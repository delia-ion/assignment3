{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Maze import Maze\n",
    "from Agent import Agent\n",
    "from MyQLearning import MyQLearning\n",
    "from MyEGreedy import MyEGreedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready reading maze file ../data/easy_maze.txt\n",
      "Made EGreedy\n",
      "Episode 0: 312 steps\n",
      "Episode 50: 62 steps\n",
      "Episode 100: 44 steps\n",
      "Episode 150: 44 steps\n",
      "Stopping after 201 episodes\n",
      "Optimal path:\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "# Load the maze\n",
    "file = \"../data/easy_maze.txt\"\n",
    "maze = Maze(file)\n",
    "\n",
    "# Set the reward at the bottom right to 10\n",
    "maze.set_reward(maze.get_state(24, 14), 10)\n",
    "\n",
    "# Create a robot at starting and reset location (0,0) (top left)\n",
    "robot = Agent(0, 0)\n",
    "\n",
    "# Parameters\n",
    "alfa = 0.7\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "max_episodes = 5000\n",
    "max_steps = 2000\n",
    "consecutive_episodes = 100\n",
    "max_steps_threshold = 150  # Threshold for stopping criterion\n",
    "rolling_avg_window = 200\n",
    "\n",
    "# Initialize objects\n",
    "selection = MyEGreedy()\n",
    "learn = MyQLearning()\n",
    "\n",
    "consecutive_episode_count = 0\n",
    "final_episode_steps = 0\n",
    "episode_lengths = []\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    state = robot.get_state(maze)\n",
    "    steps = 0\n",
    "\n",
    "    while steps < max_steps:\n",
    "        action = selection.get_egreedy_action(robot, maze, learn, epsilon)\n",
    "        state_next = robot.do_action(action, maze)\n",
    "        r = maze.get_reward(state_next)\n",
    "        possible_actions = maze.get_valid_actions(robot)\n",
    "\n",
    "        learn.update_q(state, action, r, state_next, possible_actions, alfa, gamma)\n",
    "\n",
    "        state = state_next\n",
    "        steps += 1\n",
    "\n",
    "        if state == maze.get_state(24, 14):  # reached goal\n",
    "            robot.reset()\n",
    "            break\n",
    "\n",
    "    episode_lengths.append(steps)\n",
    "\n",
    "    if episode >= rolling_avg_window:\n",
    "        rolling_avg = sum(episode_lengths[-rolling_avg_window:]) / rolling_avg_window\n",
    "        if rolling_avg <= max_steps_threshold:\n",
    "            print(f\"Stopping after {episode + 1} episodes\")\n",
    "            final_episode_steps = steps\n",
    "            break\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        print(f\"Episode {episode}: {steps} steps\")\n",
    "\n",
    "print(\"Optimal path:\")\n",
    "print(final_episode_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready reading maze file ../data/easy_maze.txt\n",
      "Made EGreedy\n",
      "Run 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_q'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 40\u001B[0m\n\u001B[1;32m     37\u001B[0m steps \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m steps \u001B[38;5;241m<\u001B[39m max_steps:\n\u001B[0;32m---> 40\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[43mselection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_egreedy_action\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrobot\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmaze\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m     state_next \u001B[38;5;241m=\u001B[39m robot\u001B[38;5;241m.\u001B[39mdo_action(action, maze)\n\u001B[1;32m     42\u001B[0m     r \u001B[38;5;241m=\u001B[39m maze\u001B[38;5;241m.\u001B[39mget_reward(state_next)\n",
      "File \u001B[0;32m~/Documents/Y2/Q3/Computation Intelligence/assignment3/src/MyEGreedy.py:42\u001B[0m, in \u001B[0;36mMyEGreedy.get_egreedy_action\u001B[0;34m(self, robot, maze, learn, epsilon)\u001B[0m\n\u001B[1;32m     40\u001B[0m best_q \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39mnp\u001B[38;5;241m.\u001B[39minf\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m valid_actions:\n\u001B[0;32m---> 42\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mlearn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_q\u001B[49m(robot\u001B[38;5;241m.\u001B[39mget_state(maze), a) \u001B[38;5;241m>\u001B[39m best_q:\n\u001B[1;32m     43\u001B[0m         best_q \u001B[38;5;241m=\u001B[39m learn\u001B[38;5;241m.\u001B[39mget_q(robot\u001B[38;5;241m.\u001B[39mget_state(maze), a)\n\u001B[1;32m     44\u001B[0m         action \u001B[38;5;241m=\u001B[39m a\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'get_q'"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.10 (tags/v3.9.10:f2f3f53, Jan 17 2022, 15:14:21) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
